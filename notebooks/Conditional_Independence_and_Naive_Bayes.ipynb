{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0b377b",
   "metadata": {},
   "source": [
    "# Conditional Independence and Naive Bayes Experiments\n",
    "\n",
    "This notebook explores the concept of conditional independence and its practical application in Naive Bayes classifiers. We'll conduct several experiments to understand:\n",
    "\n",
    "1. **Conditional Independence Theory**: What it means and how to test it\n",
    "2. **Graphical Models**: Visualizing independence relationships\n",
    "3. **Naive Bayes Implementation**: From scratch and using scikit-learn\n",
    "4. **Real-world Applications**: Text classification and medical diagnosis examples\n",
    "5. **Performance Analysis**: When Naive Bayes works well despite violated assumptions\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import make_classification, fetch_20newsgroups\n",
    "\n",
    "# For graphical models visualization\n",
    "import networkx as nx\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba437998",
   "metadata": {},
   "source": [
    "## 1. Understanding Conditional Independence\n",
    "\n",
    "Conditional independence is a fundamental concept stating that two variables A and B are conditionally independent given C if:\n",
    "\n",
    "$$P(A, B | C) = P(A | C) \\cdot P(B | C)$$\n",
    "\n",
    "Or equivalently:\n",
    "$$P(A | B, C) = P(A | C)$$\n",
    "\n",
    "Let's create a simple example to demonstrate this concept with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb242bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic medical data to demonstrate conditional independence.\n",
    "    \n",
    "    Scenario: Patient diagnosis\n",
    "    - C: Has infection (hidden cause)\n",
    "    - A: Has fever (symptom)\n",
    "    - B: Lab test positive (diagnostic test)\n",
    "    \n",
    "    Both fever and lab test depend on infection status, but are\n",
    "    conditionally independent given the infection status.\n",
    "    \"\"\"\n",
    "    # Generate infection status (30% of patients have infection)\n",
    "    infection = np.random.binomial(1, 0.3, n_samples)\n",
    "    \n",
    "    # Generate fever based on infection (90% with infection have fever, 10% without infection have fever)\n",
    "    fever_prob = np.where(infection == 1, 0.9, 0.1)\n",
    "    fever = np.random.binomial(1, fever_prob)\n",
    "    \n",
    "    # Generate lab test results based on infection (95% with infection test positive, 5% without infection test positive)\n",
    "    lab_prob = np.where(infection == 1, 0.95, 0.05)\n",
    "    lab_positive = np.random.binomial(1, lab_prob)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'infection': infection,\n",
    "        'fever': fever,\n",
    "        'lab_positive': lab_positive\n",
    "    })\n",
    "\n",
    "# Generate the data\n",
    "medical_data = generate_medical_data(5000)\n",
    "print(\"Medical data sample:\")\n",
    "print(medical_data.head(10))\n",
    "print(f\"\\nData shape: {medical_data.shape}\")\n",
    "print(f\"\\nInfection rate: {medical_data['infection'].mean():.3f}\")\n",
    "print(f\"Fever rate: {medical_data['fever'].mean():.3f}\")\n",
    "print(f\"Lab positive rate: {medical_data['lab_positive'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conditional_independence(data, var_a, var_b, condition_var):\n",
    "    \"\"\"\n",
    "    Test conditional independence using chi-squared test.\n",
    "    \n",
    "    Tests if var_a and var_b are conditionally independent given condition_var.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Test overall dependence (without conditioning)\n",
    "    contingency_table = pd.crosstab(data[var_a], data[var_b])\n",
    "    chi2, p_val_overall, dof, expected = chi2_contingency(contingency_table)\n",
    "    results['overall'] = {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_val_overall,\n",
    "        'dependent': p_val_overall < 0.05\n",
    "    }\n",
    "    \n",
    "    # Test conditional independence for each value of conditioning variable\n",
    "    condition_tests = {}\n",
    "    for condition_value in data[condition_var].unique():\n",
    "        subset = data[data[condition_var] == condition_value]\n",
    "        if len(subset) > 10:  # Need sufficient data\n",
    "            try:\n",
    "                cont_table = pd.crosstab(subset[var_a], subset[var_b])\n",
    "                if cont_table.shape == (2, 2):  # Only for 2x2 tables\n",
    "                    chi2_cond, p_val_cond, _, _ = chi2_contingency(cont_table)\n",
    "                    condition_tests[condition_value] = {\n",
    "                        'chi2': chi2_cond,\n",
    "                        'p_value': p_val_cond,\n",
    "                        'dependent': p_val_cond < 0.05,\n",
    "                        'sample_size': len(subset)\n",
    "                    }\n",
    "            except ValueError:\n",
    "                # Handle cases where chi2 test can't be performed\n",
    "                condition_tests[condition_value] = {\n",
    "                    'chi2': None,\n",
    "                    'p_value': None,\n",
    "                    'dependent': None,\n",
    "                    'sample_size': len(subset)\n",
    "                }\n",
    "    \n",
    "    results['conditional'] = condition_tests\n",
    "    return results\n",
    "\n",
    "# Test conditional independence in our medical data\n",
    "ci_results = test_conditional_independence(medical_data, 'fever', 'lab_positive', 'infection')\n",
    "\n",
    "print(\"CONDITIONAL INDEPENDENCE TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Overall dependence between fever and lab_positive:\")\n",
    "print(f\"  Chi-squared: {ci_results['overall']['chi2']:.4f}\")\n",
    "print(f\"  P-value: {ci_results['overall']['p_value']:.6f}\")\n",
    "print(f\"  Dependent: {ci_results['overall']['dependent']}\")\n",
    "print()\n",
    "\n",
    "print(\"Conditional independence tests:\")\n",
    "for condition, result in ci_results['conditional'].items():\n",
    "    condition_name = \"No Infection\" if condition == 0 else \"Has Infection\"\n",
    "    print(f\"  Given {condition_name} (n={result['sample_size']}):\")\n",
    "    if result['p_value'] is not None:\n",
    "        print(f\"    Chi-squared: {result['chi2']:.4f}\")\n",
    "        print(f\"    P-value: {result['p_value']:.6f}\")\n",
    "        print(f\"    Dependent: {result['dependent']}\")\n",
    "    else:\n",
    "        print(f\"    Could not perform test (insufficient data)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f28487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the conditional independence relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Joint distribution heatmap\n",
    "joint_prob = pd.crosstab(medical_data['fever'], medical_data['lab_positive'], normalize='all')\n",
    "sns.heatmap(joint_prob, annot=True, fmt='.3f', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Joint P(Fever, Lab+)')\n",
    "axes[0,0].set_xlabel('Lab Positive')\n",
    "axes[0,0].set_ylabel('Fever')\n",
    "\n",
    "# 2. Conditional distributions given infection status\n",
    "infection_groups = medical_data.groupby('infection')\n",
    "\n",
    "# For no infection group\n",
    "no_infection = medical_data[medical_data['infection'] == 0]\n",
    "if len(no_infection) > 0:\n",
    "    cond_prob_no_inf = pd.crosstab(no_infection['fever'], no_infection['lab_positive'], normalize='all')\n",
    "    sns.heatmap(cond_prob_no_inf, annot=True, fmt='.3f', cmap='Reds', ax=axes[0,1])\n",
    "    axes[0,1].set_title('P(Fever, Lab+ | No Infection)')\n",
    "    axes[0,1].set_xlabel('Lab Positive')\n",
    "    axes[0,1].set_ylabel('Fever')\n",
    "\n",
    "# For infection group\n",
    "has_infection = medical_data[medical_data['infection'] == 1]\n",
    "if len(has_infection) > 0:\n",
    "    cond_prob_inf = pd.crosstab(has_infection['fever'], has_infection['lab_positive'], normalize='all')\n",
    "    sns.heatmap(cond_prob_inf, annot=True, fmt='.3f', cmap='Greens', ax=axes[1,0])\n",
    "    axes[1,0].set_title('P(Fever, Lab+ | Has Infection)')\n",
    "    axes[1,0].set_xlabel('Lab Positive')\n",
    "    axes[1,0].set_ylabel('Fever')\n",
    "\n",
    "# 3. Correlation analysis\n",
    "correlations = []\n",
    "for infection_status in [0, 1]:\n",
    "    subset = medical_data[medical_data['infection'] == infection_status]\n",
    "    if len(subset) > 1:\n",
    "        corr = subset[['fever', 'lab_positive']].corr().iloc[0, 1]\n",
    "        correlations.append(corr)\n",
    "    else:\n",
    "        correlations.append(0)\n",
    "\n",
    "axes[1,1].bar(['No Infection', 'Has Infection'], correlations, \n",
    "              color=['red', 'green'], alpha=0.7)\n",
    "axes[1,1].set_title('Correlation between Fever and Lab+ by Infection Status')\n",
    "axes[1,1].set_ylabel('Correlation Coefficient')\n",
    "axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print conditional probabilities\n",
    "print(\"CONDITIONAL PROBABILITY ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "for infection_status in [0, 1]:\n",
    "    status_name = \"No Infection\" if infection_status == 0 else \"Has Infection\"\n",
    "    subset = medical_data[medical_data['infection'] == infection_status]\n",
    "    \n",
    "    print(f\"\\nGiven {status_name}:\")\n",
    "    # P(Fever=1, Lab+=1 | Infection)\n",
    "    prob_both = len(subset[(subset['fever'] == 1) & (subset['lab_positive'] == 1)]) / len(subset)\n",
    "    # P(Fever=1 | Infection) * P(Lab+=1 | Infection)\n",
    "    prob_fever = subset['fever'].mean()\n",
    "    prob_lab = subset['lab_positive'].mean()\n",
    "    prob_product = prob_fever * prob_lab\n",
    "    \n",
    "    print(f\"  P(Fever=1, Lab+=1 | {status_name}) = {prob_both:.4f}\")\n",
    "    print(f\"  P(Fever=1 | {status_name}) * P(Lab+=1 | {status_name}) = {prob_fever:.4f} * {prob_lab:.4f} = {prob_product:.4f}\")\n",
    "    print(f\"  Difference: {abs(prob_both - prob_product):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b16f96",
   "metadata": {},
   "source": [
    "## 2. Graphical Models and Independence Patterns\n",
    "\n",
    "Graphical models help us visualize and understand independence relationships. There are three fundamental patterns:\n",
    "\n",
    "1. **Common Cause (Tail-to-tail)**: A ← C → B\n",
    "2. **Chain (Head-to-tail)**: A → C → B  \n",
    "3. **Common Effect (Head-to-head)**: A → C ← B\n",
    "\n",
    "Let's visualize these patterns and understand their independence implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b92cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graphical_model(edges, title, pos=None):\n",
    "    \"\"\"Draw a simple graphical model.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "            node_size=2000, font_size=16, font_weight='bold',\n",
    "            arrows=True, arrowsize=20, edge_color='gray', arrowstyle='->')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Draw the three fundamental patterns\n",
    "print(\"Three Fundamental Graphical Model Patterns:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Pattern 1: Common Cause (our medical example)\n",
    "print(\"\\n1. Common Cause Pattern (Tail-to-tail): Infection → Fever, Infection → Lab+\")\n",
    "edges1 = [('Infection', 'Fever'), ('Infection', 'Lab+')]\n",
    "pos1 = {'Infection': (0.5, 1), 'Fever': (0, 0), 'Lab+': (1, 0)}\n",
    "draw_graphical_model(edges1, \"Common Cause: A ← C → B\\n(Fever ← Infection → Lab+)\", pos1)\n",
    "\n",
    "print(\"Independence: Fever ⊥ Lab+ | Infection\")\n",
    "print(\"Fever and Lab+ are conditionally independent given Infection status\")\n",
    "\n",
    "# Pattern 2: Chain\n",
    "print(\"\\n2. Chain Pattern (Head-to-tail): Weather → Umbrella Sales → Store Revenue\")\n",
    "edges2 = [('Weather', 'Umbrella Sales'), ('Umbrella Sales', 'Store Revenue')]\n",
    "pos2 = {'Weather': (0, 0), 'Umbrella Sales': (0.5, 0), 'Store Revenue': (1, 0)}\n",
    "draw_graphical_model(edges2, \"Chain: A → C → B\\n(Weather → Umbrella Sales → Store Revenue)\", pos2)\n",
    "\n",
    "print(\"Independence: Weather ⊥ Store Revenue | Umbrella Sales\")\n",
    "print(\"Weather and Store Revenue are conditionally independent given Umbrella Sales\")\n",
    "\n",
    "# Pattern 3: Common Effect\n",
    "print(\"\\n3. Common Effect Pattern (Head-to-head): Exercise → Health ← Diet\")\n",
    "edges3 = [('Exercise', 'Health'), ('Diet', 'Health')]\n",
    "pos3 = {'Exercise': (0, 0), 'Diet': (1, 0), 'Health': (0.5, -0.5)}\n",
    "draw_graphical_model(edges3, \"Common Effect: A → C ← B\\n(Exercise → Health ← Diet)\", pos3)\n",
    "\n",
    "print(\"Independence: Exercise ⊥ Diet (unconditionally)\")\n",
    "print(\"But Exercise ⊥̸ Diet | Health (dependent when Health is observed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7757b",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes Classifier from Scratch\n",
    "\n",
    "The Naive Bayes classifier makes the \"naive\" assumption that all features are conditionally independent given the class label:\n",
    "\n",
    "$$P(x_1, x_2, ..., x_n | y) = \\prod_{i=1}^{n} P(x_i | y)$$\n",
    "\n",
    "Using Bayes' theorem:\n",
    "$$P(y | x_1, x_2, ..., x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i | y)}{P(x_1, x_2, ..., x_n)}$$\n",
    "\n",
    "Let's implement a Gaussian Naive Bayes classifier from scratch and test it on our medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee83e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesFromScratch:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implemented from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.feature_stats = {}  # Will store mean and std for each feature per class\n",
    "        self.classes = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Naive Bayes classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        y: Target labels (n_samples,)\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Calculate class priors P(y)\n",
    "        for class_label in self.classes:\n",
    "            class_count = np.sum(y == class_label)\n",
    "            self.class_priors[class_label] = class_count / n_samples\n",
    "        \n",
    "        # Calculate feature statistics for each class\n",
    "        self.feature_stats = {}\n",
    "        for class_label in self.classes:\n",
    "            # Get samples for this class\n",
    "            class_mask = (y == class_label)\n",
    "            class_features = X[class_mask]\n",
    "            \n",
    "            # Calculate mean and std for each feature\n",
    "            self.feature_stats[class_label] = {\n",
    "                'mean': np.mean(class_features, axis=0),\n",
    "                'std': np.std(class_features, axis=0) + 1e-6  # Add small value to avoid division by zero\n",
    "            }\n",
    "    \n",
    "    def _gaussian_probability(self, x, mean, std):\n",
    "        \"\"\"Calculate Gaussian probability density.\"\"\"\n",
    "        exponent = -0.5 * ((x - mean) / std) ** 2\n",
    "        return (1 / (std * np.sqrt(2 * np.pi))) * np.exp(exponent)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        class_probabilities = {}\n",
    "        \n",
    "        for class_label in self.classes:\n",
    "            # Start with prior probability\n",
    "            prob = self.class_priors[class_label]\n",
    "            \n",
    "            # Multiply by likelihood of each feature (naive assumption)\n",
    "            stats = self.feature_stats[class_label]\n",
    "            for i, feature_value in enumerate(x):\n",
    "                feature_prob = self._gaussian_probability(\n",
    "                    feature_value, stats['mean'][i], stats['std'][i]\n",
    "                )\n",
    "                prob *= feature_prob\n",
    "            \n",
    "            class_probabilities[class_label] = prob\n",
    "        \n",
    "        # Return class with highest probability\n",
    "        return max(class_probabilities, key=class_probabilities.get)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for multiple samples.\"\"\"\n",
    "        X = np.array(X)\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self._predict_single(x))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for multiple samples.\"\"\"\n",
    "        X = np.array(X)\n",
    "        probabilities = []\n",
    "        \n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for class_label in self.classes:\n",
    "                prob = self.class_priors[class_label]\n",
    "                stats = self.feature_stats[class_label]\n",
    "                for i, feature_value in enumerate(x):\n",
    "                    feature_prob = self._gaussian_probability(\n",
    "                        feature_value, stats['mean'][i], stats['std'][i]\n",
    "                    )\n",
    "                    prob *= feature_prob\n",
    "                class_probs[class_label] = prob\n",
    "            \n",
    "            # Normalize probabilities\n",
    "            total_prob = sum(class_probs.values())\n",
    "            normalized_probs = [class_probs[class_label] / total_prob for class_label in self.classes]\n",
    "            probabilities.append(normalized_probs)\n",
    "        \n",
    "        return np.array(probabilities)\n",
    "\n",
    "# Test our implementation on medical data\n",
    "print(\"Testing Naive Bayes from Scratch on Medical Data\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare data\n",
    "X = medical_data[['fever', 'lab_positive']].values\n",
    "y = medical_data['infection'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train our model\n",
    "nb_scratch = NaiveBayesFromScratch()\n",
    "nb_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_scratch = nb_scratch.predict(X_test)\n",
    "y_proba_scratch = nb_scratch.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(f\"Accuracy (from scratch): {accuracy_scratch:.4f}\")\n",
    "\n",
    "# Compare with scikit-learn\n",
    "nb_sklearn = GaussianNB()\n",
    "nb_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = nb_sklearn.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Accuracy (scikit-learn): {accuracy_sklearn:.4f}\")\n",
    "\n",
    "print(f\"\\nClass priors learned by our model:\")\n",
    "for class_label, prior in nb_scratch.class_priors.items():\n",
    "    class_name = \"No Infection\" if class_label == 0 else \"Has Infection\"\n",
    "    print(f\"  {class_name}: {prior:.4f}\")\n",
    "\n",
    "print(f\"\\nFeature statistics learned by our model:\")\n",
    "for class_label in nb_scratch.classes:\n",
    "    class_name = \"No Infection\" if class_label == 0 else \"Has Infection\"\n",
    "    stats = nb_scratch.feature_stats[class_label]\n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    Fever - Mean: {stats['mean'][0]:.4f}, Std: {stats['std'][0]:.4f}\")\n",
    "    print(f\"    Lab+ - Mean: {stats['mean'][1]:.4f}, Std: {stats['std'][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef94124",
   "metadata": {},
   "source": [
    "## 4. Text Classification with Naive Bayes\n",
    "\n",
    "Naive Bayes is particularly effective for text classification. Let's implement a sentiment analysis system and explore why the \"naive\" assumption works well despite being violated in practice.\n",
    "\n",
    "We'll use a simple movie review dataset and compare different variants of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple movie review dataset\n",
    "def create_movie_review_data():\n",
    "    \"\"\"Create synthetic movie review data for sentiment analysis.\"\"\"\n",
    "    \n",
    "    positive_reviews = [\n",
    "        \"This movie was absolutely fantastic and amazing\",\n",
    "        \"I loved every minute of this incredible film\",\n",
    "        \"Outstanding performance and brilliant storytelling\",\n",
    "        \"A masterpiece of cinema with excellent acting\",\n",
    "        \"Wonderful plot and spectacular visuals\",\n",
    "        \"This film exceeded all my expectations perfectly\",\n",
    "        \"Brilliant direction and amazing cinematography\",\n",
    "        \"Absolutely loved the characters and story\",\n",
    "        \"An incredible journey with perfect ending\",\n",
    "        \"Fantastic movie with outstanding performances\",\n",
    "        \"Amazing plot twists and excellent writing\",\n",
    "        \"Perfect blend of action and emotion\",\n",
    "        \"Wonderful acting and brilliant dialogue\",\n",
    "        \"This movie was truly spectacular\",\n",
    "        \"Excellent film with amazing visual effects\"\n",
    "    ]\n",
    "    \n",
    "    negative_reviews = [\n",
    "        \"This movie was terrible and completely boring\",\n",
    "        \"I hated every minute of this awful film\",\n",
    "        \"Poor performance and terrible storytelling\",\n",
    "        \"A disaster of cinema with horrible acting\",\n",
    "        \"Awful plot and terrible visuals\",\n",
    "        \"This film disappointed me completely\",\n",
    "        \"Poor direction and terrible cinematography\",\n",
    "        \"Absolutely hated the characters and story\",\n",
    "        \"A boring journey with awful ending\",\n",
    "        \"Terrible movie with poor performances\",\n",
    "        \"Awful plot twists and poor writing\",\n",
    "        \"Horrible blend of action and emotion\",\n",
    "        \"Poor acting and terrible dialogue\",\n",
    "        \"This movie was truly awful\",\n",
    "        \"Terrible film with poor visual effects\"\n",
    "    ]\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "    texts = positive_reviews + negative_reviews\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# Generate the dataset\n",
    "texts, labels = create_movie_review_data()\n",
    "print(f\"Created dataset with {len(texts)} reviews\")\n",
    "print(f\"Positive reviews: {sum(labels)}\")\n",
    "print(f\"Negative reviews: {len(labels) - sum(labels)}\")\n",
    "\n",
    "print(\"\\nSample reviews:\")\n",
    "print(\"Positive:\", texts[0])\n",
    "print(\"Negative:\", texts[15])\n",
    "\n",
    "# Prepare data for machine learning\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train_text)} reviews\")\n",
    "print(f\"Test set: {len(X_test_text)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ae7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different Naive Bayes variants for text classification\n",
    "def compare_naive_bayes_variants(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Compare different Naive Bayes variants on text data.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Multinomial Naive Bayes with Count Vectorizer\n",
    "    count_vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "    X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_count = count_vectorizer.transform(X_test)\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train_count, y_train)\n",
    "    mnb_pred = mnb.predict(X_test_count)\n",
    "    mnb_accuracy = accuracy_score(y_test, mnb_pred)\n",
    "    results['Multinomial NB (Count)'] = mnb_accuracy\n",
    "    \n",
    "    # 2. Multinomial Naive Bayes with TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    mnb_tfidf = MultinomialNB()\n",
    "    mnb_tfidf.fit(X_train_tfidf, y_train)\n",
    "    mnb_tfidf_pred = mnb_tfidf.predict(X_test_tfidf)\n",
    "    mnb_tfidf_accuracy = accuracy_score(y_test, mnb_tfidf_pred)\n",
    "    results['Multinomial NB (TF-IDF)'] = mnb_tfidf_accuracy\n",
    "    \n",
    "    # 3. Bernoulli Naive Bayes\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train_count, y_train)\n",
    "    bnb_pred = bnb.predict(X_test_count)\n",
    "    bnb_accuracy = accuracy_score(y_test, bnb_pred)\n",
    "    results['Bernoulli NB'] = bnb_accuracy\n",
    "    \n",
    "    return results, count_vectorizer, mnb\n",
    "\n",
    "# Test different variants\n",
    "nb_results, vectorizer, best_model = compare_naive_bayes_variants(\n",
    "    X_train_text, X_test_text, y_train_text, y_test_text\n",
    ")\n",
    "\n",
    "print(\"NAIVE BAYES VARIANTS COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "for model_name, accuracy in nb_results.items():\n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "# Analyze the learned vocabulary\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\nVocabulary size: {len(feature_names)}\")\n",
    "print(f\"Sample features: {feature_names[:10]}\")\n",
    "\n",
    "# Get most informative features\n",
    "def get_most_informative_features(model, vectorizer, n_features=10):\n",
    "    \"\"\"Get the most informative features for each class.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get log probabilities for each class\n",
    "    class_0_log_probs = model.feature_log_prob_[0]  # Negative class\n",
    "    class_1_log_probs = model.feature_log_prob_[1]  # Positive class\n",
    "    \n",
    "    # Calculate the ratio of probabilities\n",
    "    log_prob_ratio = class_1_log_probs - class_0_log_probs\n",
    "    \n",
    "    # Get top features for positive class\n",
    "    positive_indices = np.argsort(log_prob_ratio)[-n_features:][::-1]\n",
    "    positive_features = [(feature_names[i], log_prob_ratio[i]) for i in positive_indices]\n",
    "    \n",
    "    # Get top features for negative class\n",
    "    negative_indices = np.argsort(log_prob_ratio)[:n_features]\n",
    "    negative_features = [(feature_names[i], log_prob_ratio[i]) for i in negative_indices]\n",
    "    \n",
    "    return positive_features, negative_features\n",
    "\n",
    "positive_features, negative_features = get_most_informative_features(best_model, vectorizer)\n",
    "\n",
    "print(\"\\nMOST INFORMATIVE FEATURES\")\n",
    "print(\"=\"*30)\n",
    "print(\"Top features for POSITIVE sentiment:\")\n",
    "for feature, ratio in positive_features:\n",
    "    print(f\"  {feature}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\nTop features for NEGATIVE sentiment:\")\n",
    "for feature, ratio in negative_features:\n",
    "    print(f\"  {feature}: {ratio:.4f}\")\n",
    "\n",
    "# Test predictions on new examples\n",
    "test_examples = [\n",
    "    \"This movie was absolutely fantastic\",\n",
    "    \"Terrible film with awful acting\",\n",
    "    \"Amazing story and brilliant performance\",\n",
    "    \"Boring and disappointing movie\"\n",
    "]\n",
    "\n",
    "X_test_examples = vectorizer.transform(test_examples)\n",
    "predictions = best_model.predict(X_test_examples)\n",
    "probabilities = best_model.predict_proba(X_test_examples)\n",
    "\n",
    "print(\"\\nPREDICTIONS ON NEW EXAMPLES\")\n",
    "print(\"=\"*35)\n",
    "for i, (text, pred, prob) in enumerate(zip(test_examples, predictions, probabilities)):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = max(prob)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715366a",
   "metadata": {},
   "source": [
    "## 5. Analyzing the \"Naive\" Assumption\n",
    "\n",
    "The \"naive\" assumption in Naive Bayes is that features are conditionally independent given the class. In reality, this assumption is often violated. Let's explore:\n",
    "\n",
    "1. How to detect when the assumption is violated\n",
    "2. Why Naive Bayes still works well despite violated assumptions\n",
    "3. When the assumption matters most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bbc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_dependencies(X, y, feature_names, n_features=5):\n",
    "    \"\"\"\n",
    "    Analyze dependencies between features within each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for class_label in np.unique(y):\n",
    "        print(f\"\\nANALYZING CLASS {class_label}\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        # Get data for this class\n",
    "        class_mask = (y == class_label)\n",
    "        X_class = X[class_mask]\n",
    "        \n",
    "        if hasattr(X_class, 'toarray'):  # Handle sparse matrices\n",
    "            X_class = X_class.toarray()\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        correlations = np.corrcoef(X_class.T)\n",
    "        \n",
    "        # Find highly correlated feature pairs\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(feature_names)):\n",
    "            for j in range(i+1, len(feature_names)):\n",
    "                if abs(correlations[i, j]) > 0.3:  # Threshold for high correlation\n",
    "                    high_corr_pairs.append((\n",
    "                        feature_names[i], \n",
    "                        feature_names[j], \n",
    "                        correlations[i, j]\n",
    "                    ))\n",
    "        \n",
    "        # Sort by correlation strength\n",
    "        high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "        \n",
    "        print(f\"Highly correlated feature pairs (|r| > 0.3):\")\n",
    "        for feat1, feat2, corr in high_corr_pairs[:n_features]:\n",
    "            print(f\"  {feat1} <-> {feat2}: {corr:.4f}\")\n",
    "        \n",
    "        if not high_corr_pairs:\n",
    "            print(\"  No highly correlated feature pairs found\")\n",
    "        \n",
    "        results[class_label] = high_corr_pairs\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze feature dependencies in our text data\n",
    "print(\"FEATURE DEPENDENCY ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get feature matrix and analyze\n",
    "X_count = vectorizer.transform(X_train_text + X_test_text)\n",
    "y_all = np.array(y_train_text + y_test_text)\n",
    "\n",
    "# Select top features to make analysis manageable\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_features_idx = np.argsort(np.array(X_count.sum(axis=0)).flatten())[-20:]  # Top 20 most frequent\n",
    "top_feature_names = feature_names[top_features_idx]\n",
    "X_count_top = X_count[:, top_features_idx]\n",
    "\n",
    "dependencies = analyze_feature_dependencies(X_count_top, y_all, top_feature_names)\n",
    "\n",
    "# Create a synthetic dataset where independence assumption is clearly violated\n",
    "def create_dependent_features_dataset():\n",
    "    \"\"\"Create a dataset where features are clearly dependent.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Class labels\n",
    "    y = np.random.binomial(1, 0.5, n_samples)\n",
    "    \n",
    "    # Feature 1: depends on class\n",
    "    f1 = np.where(y == 1, \n",
    "                  np.random.normal(2, 1, n_samples), \n",
    "                  np.random.normal(-2, 1, n_samples))\n",
    "    \n",
    "    # Feature 2: depends on both class AND feature 1 (violates independence)\n",
    "    f2 = 0.8 * f1 + np.where(y == 1, \n",
    "                             np.random.normal(1, 0.5, n_samples),\n",
    "                             np.random.normal(-1, 0.5, n_samples))\n",
    "    \n",
    "    # Feature 3: independent given class (follows naive assumption)\n",
    "    f3 = np.where(y == 1,\n",
    "                  np.random.normal(1, 1, n_samples),\n",
    "                  np.random.normal(-1, 1, n_samples))\n",
    "    \n",
    "    X = np.column_stack([f1, f2, f3])\n",
    "    return X, y\n",
    "\n",
    "# Test Naive Bayes on dependent features\n",
    "print(\"\\n\\nTESTING ON DATASET WITH DEPENDENT FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_dep, y_dep = create_dependent_features_dataset()\n",
    "X_train_dep, X_test_dep, y_train_dep, y_test_dep = train_test_split(\n",
    "    X_dep, y_dep, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb_dep = GaussianNB()\n",
    "nb_dep.fit(X_train_dep, y_train_dep)\n",
    "y_pred_dep = nb_dep.predict(X_test_dep)\n",
    "accuracy_dep = accuracy_score(y_test_dep, y_pred_dep)\n",
    "\n",
    "print(f\"Naive Bayes accuracy on dependent features: {accuracy_dep:.4f}\")\n",
    "\n",
    "# Analyze feature correlations\n",
    "print(\"\\nFeature correlations within each class:\")\n",
    "for class_label in [0, 1]:\n",
    "    class_data = X_dep[y_dep == class_label]\n",
    "    corr_matrix = np.corrcoef(class_data.T)\n",
    "    print(f\"\\nClass {class_label}:\")\n",
    "    print(f\"  Feature 1 vs Feature 2: {corr_matrix[0, 1]:.4f}\")\n",
    "    print(f\"  Feature 1 vs Feature 3: {corr_matrix[0, 2]:.4f}\")\n",
    "    print(f\"  Feature 2 vs Feature 3: {corr_matrix[1, 2]:.4f}\")\n",
    "\n",
    "# Compare with a method that can handle dependencies (like Logistic Regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_dep, y_train_dep)\n",
    "y_pred_lr = lr.predict(X_test_dep)\n",
    "accuracy_lr = accuracy_score(y_test_dep, y_pred_lr)\n",
    "\n",
    "print(f\"\\nLogistic Regression accuracy: {accuracy_lr:.4f}\")\n",
    "print(f\"Performance difference: {accuracy_lr - accuracy_dep:.4f}\")\n",
    "\n",
    "# Visualize the dependent features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot relationships between features colored by class\n",
    "colors = ['red', 'blue']\n",
    "class_names = ['Class 0', 'Class 1']\n",
    "\n",
    "for i, class_label in enumerate([0, 1]):\n",
    "    mask = y_dep == class_label\n",
    "    axes[0].scatter(X_dep[mask, 0], X_dep[mask, 1], \n",
    "                   c=colors[i], alpha=0.6, label=class_names[i])\n",
    "    axes[1].scatter(X_dep[mask, 0], X_dep[mask, 2], \n",
    "                   c=colors[i], alpha=0.6, label=class_names[i])\n",
    "    axes[2].scatter(X_dep[mask, 1], X_dep[mask, 2], \n",
    "                   c=colors[i], alpha=0.6, label=class_names[i])\n",
    "\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2 (Dependent on F1)')\n",
    "axes[0].set_title('F1 vs F2: Clear Dependence')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 3 (Independent)')\n",
    "axes[1].set_title('F1 vs F3: Independence')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].set_xlabel('Feature 2')\n",
    "axes[2].set_ylabel('Feature 3')\n",
    "axes[2].set_title('F2 vs F3')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Explain why Naive Bayes still works reasonably well\n",
    "print(\"\\nWHY NAIVE BAYES WORKS DESPITE VIOLATED ASSUMPTIONS:\")\n",
    "print(\"=\"*55)\n",
    "print(\"1. We only need correct classification, not exact probabilities\")\n",
    "print(\"2. The decision boundary often remains reasonable\")\n",
    "print(\"3. Errors in probability estimates may cancel out\")\n",
    "print(\"4. Strong independence violations are needed for major impact\")\n",
    "print(\"5. Large datasets help overcome assumption violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91302e5",
   "metadata": {},
   "source": [
    "## 6. Real-world Application: Document Classification\n",
    "\n",
    "Let's apply our understanding to a more realistic scenario using actual newsgroup data. This will demonstrate how Naive Bayes performs on real text data where the independence assumption is clearly violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of the 20 newsgroups dataset\n",
    "try:\n",
    "    # Select a few categories for faster processing\n",
    "    categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "    \n",
    "    newsgroups_train = fetch_20newsgroups(\n",
    "        subset='train',\n",
    "        categories=categories,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        remove=('headers', 'footers', 'quotes')  # Remove metadata that could leak information\n",
    "    )\n",
    "    \n",
    "    newsgroups_test = fetch_20newsgroups(\n",
    "        subset='test',\n",
    "        categories=categories,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        remove=('headers', 'footers', 'quotes')\n",
    "    )\n",
    "    \n",
    "    print(\"REAL-WORLD DOCUMENT CLASSIFICATION\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Training documents: {len(newsgroups_train.data)}\")\n",
    "    print(f\"Test documents: {len(newsgroups_test.data)}\")\n",
    "    print(f\"Categories: {newsgroups_train.target_names}\")\n",
    "    \n",
    "    # Show sample documents\n",
    "    print(\"\\nSample documents:\")\n",
    "    for i in range(len(categories)):\n",
    "        idx = np.where(newsgroups_train.target == i)[0][0]\n",
    "        print(f\"\\nCategory: {newsgroups_train.target_names[i]}\")\n",
    "        print(f\"Text (first 200 chars): {newsgroups_train.data[idx][:200]}...\")\n",
    "    \n",
    "    # Vectorize the data\n",
    "    print(\"\\nVectorizing documents...\")\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=1000,  # Limit features for faster processing\n",
    "        stop_words='english',\n",
    "        max_df=0.95,  # Remove very common words\n",
    "        min_df=2      # Remove very rare words\n",
    "    )\n",
    "    \n",
    "    X_train_news = tfidf.fit_transform(newsgroups_train.data)\n",
    "    X_test_news = tfidf.transform(newsgroups_test.data)\n",
    "    y_train_news = newsgroups_train.target\n",
    "    y_test_news = newsgroups_test.target\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X_train_news.shape}\")\n",
    "    print(f\"Vocabulary size: {len(tfidf.get_feature_names_out())}\")\n",
    "    \n",
    "    # Train Naive Bayes\n",
    "    print(\"\\nTraining Naive Bayes classifier...\")\n",
    "    nb_news = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "    nb_news.fit(X_train_news, y_train_news)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_news = nb_news.predict(X_test_news)\n",
    "    accuracy_news = accuracy_score(y_test_news, y_pred_news)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_news:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test_news, y_pred_news, \n",
    "                              target_names=newsgroups_test.target_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_news, y_pred_news)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=newsgroups_test.target_names,\n",
    "                yticklabels=newsgroups_test.target_names)\n",
    "    plt.title('Confusion Matrix - Newsgroup Classification')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most discriminative features for each category\n",
    "    print(\"\\nMost discriminative features by category:\")\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    for i, category in enumerate(newsgroups_test.target_names):\n",
    "        # Get log probabilities for this class vs others\n",
    "        class_log_probs = nb_news.feature_log_prob_[i]\n",
    "        \n",
    "        # Get top features\n",
    "        top_indices = np.argsort(class_log_probs)[-10:][::-1]\n",
    "        top_features = [feature_names[idx] for idx in top_indices]\n",
    "        \n",
    "        print(f\"\\n{category}:\")\n",
    "        print(f\"  {', '.join(top_features)}\")\n",
    "    \n",
    "    # Test on custom examples\n",
    "    test_docs = [\n",
    "        \"I don't believe in God and think religion is harmful\",\n",
    "        \"Computer graphics and image processing algorithms\",\n",
    "        \"Patient symptoms and medical diagnosis procedures\",\n",
    "        \"Jesus Christ is our savior and lord\"\n",
    "    ]\n",
    "    \n",
    "    X_custom = tfidf.transform(test_docs)\n",
    "    predictions = nb_news.predict(X_custom)\n",
    "    probabilities = nb_news.predict_proba(X_custom)\n",
    "    \n",
    "    print(\"\\nPredictions on custom documents:\")\n",
    "    print(\"=\"*40)\n",
    "    for i, (doc, pred, probs) in enumerate(zip(test_docs, predictions, probabilities)):\n",
    "        predicted_category = newsgroups_test.target_names[pred]\n",
    "        confidence = np.max(probs)\n",
    "        print(f\"\\nDocument: '{doc[:50]}...'\")\n",
    "        print(f\"Predicted: {predicted_category} (confidence: {confidence:.4f})\")\n",
    "        print(\"All probabilities:\")\n",
    "        for j, prob in enumerate(probs):\n",
    "            print(f\"  {newsgroups_test.target_names[j]}: {prob:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 20 newsgroups dataset: {e}\")\n",
    "    print(\"This might be due to network connectivity issues.\")\n",
    "    print(\"The core concepts have been demonstrated with our synthetic examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e4aab",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Key Takeaways\n",
    "\n",
    "### Summary of Experiments\n",
    "\n",
    "Through our experiments, we've demonstrated several key concepts:\n",
    "\n",
    "1. **Conditional Independence**: We showed how to test for conditional independence and visualized the concept with medical diagnosis data.\n",
    "\n",
    "2. **Graphical Models**: We explored the three fundamental patterns (common cause, chain, common effect) and their independence implications.\n",
    "\n",
    "3. **Naive Bayes Implementation**: We built a Naive Bayes classifier from scratch and compared it with scikit-learn implementations.\n",
    "\n",
    "4. **Text Classification**: We applied Naive Bayes to sentiment analysis and document classification, showing its effectiveness despite violated assumptions.\n",
    "\n",
    "5. **Assumption Analysis**: We created datasets with known dependencies and showed that Naive Bayes can still perform well even when the independence assumption is violated.\n",
    "\n",
    "### Why Naive Bayes Works Despite \"Naive\" Assumptions\n",
    "\n",
    "1. **Decision-focused**: We only need correct classification, not exact probability estimates\n",
    "2. **Robust to violations**: Small correlations between features don't significantly impact performance\n",
    "3. **Large sample benefits**: More data helps overcome assumption violations\n",
    "4. **Balanced errors**: Estimation errors in different features may cancel out\n",
    "\n",
    "### When to Use Naive Bayes\n",
    "\n",
    "**Good for:**\n",
    "- Text classification and NLP tasks\n",
    "- Small to medium datasets\n",
    "- High-dimensional data\n",
    "- Real-time applications (fast prediction)\n",
    "- Baseline models\n",
    "- When features are actually close to independent\n",
    "\n",
    "**Consider alternatives when:**\n",
    "- Features are strongly correlated\n",
    "- Very large datasets where more complex models are feasible\n",
    "- When exact probability estimates are needed\n",
    "- Non-linear relationships are important\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Choose the right variant**: Gaussian for continuous features, Multinomial for count data, Bernoulli for binary features\n",
    "2. **Use appropriate smoothing**: Laplace smoothing helps with zero probabilities\n",
    "3. **Preprocess text properly**: Remove stop words, handle rare words\n",
    "4. **Validate assumptions**: Test for independence when possible\n",
    "5. **Compare with other methods**: Don't assume Naive Bayes is always the best choice\n",
    "\n",
    "The \"naive\" assumption in Naive Bayes is often violated in practice, but the algorithm remains surprisingly effective across many domains, especially text classification. Understanding when and why it works helps us apply it more effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
